{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT代码解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from model import GPTConfig, GPT\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. config\n",
    "训练大型模型的配置文件通常包含了一系列详细的参数和设置，用于指导训练过程。以下是一些常见的内容：\n",
    "\n",
    "1. **模型参数**：\n",
    "   - 模型架构（例如，ResNet、Transformer等）\n",
    "   - 层数、隐藏单元数、过滤器大小等\n",
    "   - 激活函数类型\n",
    "   - 是否使用正则化技术（如Dropout、Batch Normalization等）\n",
    "\n",
    "2. **数据预处理**：\n",
    "   - 数据集路径\n",
    "   - 数据增强技术（如旋转、缩放、裁剪等）\n",
    "   - 输入数据的预处理步骤（归一化、标准化等）\n",
    "   - 输出数据的编码方式（类别标签、独热编码等）\n",
    "\n",
    "3. **优化器和学习率**：\n",
    "   - 优化器类型（如Adam、SGD等）\n",
    "   - 初始学习率\n",
    "   - 学习率衰减策略（如余弦退火、指数衰减等）\n",
    "   - 权重初始化方法\n",
    "\n",
    "4. **训练设置**：\n",
    "   - 批量大小（Batch Size）\n",
    "   - 训练轮数（Epochs）\n",
    "   - 每个epoch的迭代次数\n",
    "   - 验证集和测试集的划分\n",
    "\n",
    "5. **损失函数**：\n",
    "   - 损失函数类型（如交叉熵、均方误差等）\n",
    "   - 损失函数的参数\n",
    "\n",
    "6. **评估指标**：\n",
    "   - 评估模型性能的指标（如准确率、F1分数、ROC曲线等）\n",
    "   - 评估频率\n",
    "\n",
    "7. **硬件设置**：\n",
    "   - 使用的GPU或CPU数量\n",
    "   - 内存限制\n",
    "   - 训练过程中的资源监控\n",
    "\n",
    "8. **日志和模型保存**：\n",
    "   - 日志文件的保存路径\n",
    "   - 模型检查点的保存频率\n",
    "   - 最终模型的保存路径\n",
    "\n",
    "9. **分布式训练设置**（如果适用）：\n",
    "   - 分布式训练的策略（如数据并行、模型并行等）\n",
    "   - 通信协议和后端设置\n",
    "\n",
    "10. **超参数调整**（如果使用自动化工具）：\n",
    "    - 超参数搜索空间\n",
    "    - 搜索策略（如网格搜索、随机搜索、贝叶斯优化等）\n",
    "\n",
    "配置文件的具体内容会根据所使用的框架（如TensorFlow、PyTorch等）、模型类型以及任务需求而有所不同。通常，配置文件的目的是为了确保实验的可重复性，同时方便地进行不同设置的实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================日志与模型保存=====================================\n",
    "out_dir = 'out-test'\n",
    "eval_interval = 500\n",
    "log_interval = 4\n",
    "eval_iters = 10\n",
    "always_save_checkpoint = False  # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False  # disabled by default\n",
    "wandb_project = 'shakespeare-char'\n",
    "wandb_run_name = 'mini-gpt'  # 'run' + str(time.time())\n",
    "\n",
    "# ===================================数据预处理===================================\n",
    "dataset = 'shakespeare_char'\n",
    "gradient_accumulation_steps = 4  # used to simulate larger batch sizes\n",
    "batch_size = 16  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 256  # text\n",
    "\n",
    "# ===================================模型参数=====================================\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.1  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "\n",
    "# ===================================优化器与学习率=====================================\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3  # max learning rate\n",
    "max_iters = 5000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 100  # how many steps to warm up for\n",
    "lr_decay_iters = 5000  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 1e-4  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# ===================================分布式训练设置=====================================\n",
    "# DDP settings\n",
    "backend = 'nccl'  # 'nccl', 'gloo', etc.\n",
    "\n",
    "# ===================================硬件设置=====================================\n",
    "# system\n",
    "# device = 'cuda'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available(\n",
    ") and torch.cuda.is_bf16_supported() else 'float16'\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith(\n",
    "    '_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_offset = 0\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "device_type = device\n",
    "ptdtype = {'float32': torch.float32,\n",
    "           'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(\n",
    "    device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/shakespeare_char'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join('data', dataset)\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=1024,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout)  # start with model_args from command line\n",
    "if meta_vocab_size is None:\n",
    "    print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 85.00M\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=65, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GPT(gptconf).to('cpu')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([16, 256])\n",
      "Y: torch.Size([16, 256])\n",
      "X[0,:10]:  tensor([ 1, 40, 43,  0, 42, 39, 51, 52, 43, 42])\n",
      "Y[0,:10]:  tensor([40, 43,  0, 42, 39, 51, 52, 43, 42,  1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.load('X.tensor').to('cpu')\n",
    "Y = torch.load('Y.tensor').to('cpu')\n",
    "print('X:', X.shape)\n",
    "print('Y:', Y.shape)\n",
    "print('X[0,:10]: ', X[0, :10])\n",
    "print('Y[0,:10]: ', Y[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPTConfig:\n",
    "batch_size = 16\n",
    "block_size = 1024  # lenght\n",
    "n_layer = 2\n",
    "n_head = 4\n",
    "n_embd = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPT前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:256 = block_size:1024\n",
      "-----embding-input-------\n",
      "词嵌入向量维n_embd =  128\n",
      "tok_emb: torch.Size([16, 256, 768])\n",
      "pos_emb: torch.Size([256, 768])\n",
      "tok_emb+pos_emb: torch.Size([16, 256, 768])\n",
      "编码后embding input: torch.Size([16, 256, 768])\n",
      "-----decoder-block-------\n",
      "n_layer: 2\n",
      "decoder layers: 12\n",
      "0\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "1\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "2\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "3\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "4\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "5\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "6\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "7\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "8\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "9\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "10\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "11\n",
      "decoder x: torch.Size([16, 256, 768])\n",
      "ln_f x: torch.Size([16, 256, 768])\n",
      "-----lm_head-------\n",
      "lm_head : torch.Size([16, 256, 65])\n",
      "lm_head输出与解码词汇量相同, meta_vocab_size= 65\n",
      "-----loss-------\n",
      "tensor(4.4368, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# class GPT(nn.Module).forward\n",
    "device = 'cpu'\n",
    "b, t = X.size()  # b是批量大小，t是序列长度\n",
    "print(\"t:{} = block_size:{}\".format(t, block_size))\n",
    "\n",
    "# 生成一个从0到t的整数序列，用于位置编码\n",
    "pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "# forward the GPT model itself\n",
    "tok_emb = model.transformer.wte(X)  # token embeddings of shape (b, t, n_embd)\n",
    "# position embeddings of shape (t, n_embd)\n",
    "pos_emb = model.transformer.wpe(pos)\n",
    "print('-----embding-input-------')\n",
    "print('词嵌入向量维n_embd = ', n_embd)\n",
    "print('tok_emb:', tok_emb.shape)\n",
    "print('pos_emb:', pos_emb.shape)\n",
    "print('tok_emb+pos_emb:', (tok_emb + pos_emb).shape)\n",
    "x = model.transformer.drop(tok_emb + pos_emb)  # pretraining\n",
    "x_enc = model.transformer.drop(tok_emb + pos_emb)\n",
    "print('编码后embding input:', x.shape)\n",
    "print('-----decoder-block-------')\n",
    "print('n_layer:', n_layer)\n",
    "print('decoder layers:', len(model.transformer.h))\n",
    "i = 0\n",
    "for block in model.transformer.h:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    x = block(x)\n",
    "    print('decoder x:', x.shape)\n",
    "x = model.transformer.ln_f(x)  # LayerNorm\n",
    "print('ln_f x:', x.shape)\n",
    "print('-----lm_head-------')\n",
    "logits = model.lm_head(x)  # 得到输出logits\n",
    "print('lm_head :', logits.shape)\n",
    "print('lm_head输出与解码词汇量相同, meta_vocab_size=', meta_vocab_size)\n",
    "print('-----loss-------')\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                       Y.view(-1), \n",
    "                       ignore_index=-1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------decoder block-------------\n",
      "Block(\n",
      "  (ln_1): LayerNorm()\n",
      "  (attn): CausalSelfAttention(\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm()\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
      "    (gelu): GELU(approximate='none')\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "layer norm : torch.Size([16, 256, 768])\n",
      "masked_self_attention : torch.Size([16, 256, 768])\n",
      "layer norm : torch.Size([16, 256, 768])\n",
      "mlp : torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "# class Block(nn.Module)\n",
    "print('-------------decoder block-------------')\n",
    "decoder_block = model.transformer.h[0]  # 从模型的解码器层中获取第一个解码器块\n",
    "print(decoder_block)\n",
    "\n",
    "# 根据上面打印的第一个解码器块的信息，分解解码器块的计算过程如下\n",
    "x = x_enc\n",
    "x_ln_1 = decoder_block.ln_1(x)\n",
    "x_attn = decoder_block.attn(x_ln_1)\n",
    "x_ln_2 = decoder_block.ln_2(x_attn)\n",
    "x_mlp = decoder_block.mlp(x_ln_2)\n",
    "\n",
    "print('layer norm :', x_ln_1.shape)\n",
    "print('masked_self_attention :', x_attn.shape)\n",
    "print('layer norm :', x_ln_2.shape)\n",
    "print('mlp :', x_mlp.shape)\n",
    "\n",
    "# 将x通过解码器块的第一个层归一化层和自注意力层，然后与原始的x相加。这是残差连接的一部分，可以帮助模型学习恒等函数，从而更深入地学习\n",
    "x = x + decoder_block.attn(decoder_block.ln_1(x))\n",
    "# 将x通过解码器块的第二个层归一化层和多层感知机层，然后与原始的x相加。这是残差连接的另一部分\n",
    "x = x + decoder_block.mlp(decoder_block.ln_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
      "  (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "如果torch>2.0.0, 是否可直接使用scaled_dot_product_attention： True\n",
      "batch:16, block:256, embed:768, \n",
      "---------------1. 将嵌入向量传播成3*n_embd--------------\n",
      "n_embed: 768\n",
      "n_embed*3: 2304\n",
      "x_liner: torch.Size([16, 256, 2304])\n",
      "---------------2. 将3*n_embd split成QKV--------------\n",
      "split: q: torch.Size([16, 256, 768])\n",
      "---------------3. 将QKV拆分 多头QKV--------------\n",
      "n_embed:768 / n_head:12 = 64 \n",
      "q: torch.Size([16, 12, 256, 64])\n",
      "Q = batch:16, n_head:4, block:256, head_embed:64 \n",
      "---------------4.多头计算attention，直接使用torch function--------------\n",
      "y: torch.Size([16, 12, 256, 64])\n",
      "---------------5.将多头注意力结果拼接--------------\n",
      "y-concat: torch.Size([16, 256, 768])\n",
      "---------------6. 增加一次前向传播--------------\n",
      "y_proj: torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# Masked Self Attention\n",
    "# 查看模型中attention的计算过程\n",
    "attention = model.transformer.h[0].attn\n",
    "print(attention)\n",
    "print(\"如果torch>2.0.0, 是否可直接使用scaled_dot_product_attention：\", attention.flash)\n",
    "\n",
    "x = x_ln_1\n",
    "B, T, C = x.size()\n",
    "# batch:16, block:256, embed:768\n",
    "print(\"batch:{}, block:{}, embed:{}, \".format(B, T, C))\n",
    "\n",
    "\n",
    "# self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "print('---------------1. 将嵌入向量传播成3*n_embd--------------')\n",
    "x_liner = attention.c_attn(x)  # 将x通过自注意力层的线性层，得到3倍嵌入维度的输出\n",
    "print(\"n_embed:\", attention.n_embd)\n",
    "print(\"n_embed*3:\", attention.n_embd*3)  # 3份分别是QKV\n",
    "print(\"x_liner:\", x_liner.shape)\n",
    "\n",
    "print('---------------2. 将3*n_embd split成QKV--------------')\n",
    "q, k, v = x_liner.split(attention.n_embd, dim=2)\n",
    "print(\"split: q:\", q.shape)\n",
    "\n",
    "print('---------------3. 将QKV拆分 多头QKV--------------')\n",
    "print(\"n_embed:{} / n_head:{} = {} \".format(C, attention.n_head, C//attention.n_head))\n",
    "k = k.view(B, T, attention.n_head, C //\n",
    "           attention.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "q = q.view(B, T, attention.n_head, C //\n",
    "           attention.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "v = v.view(B, T, attention.n_head, C //\n",
    "           attention.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"Q = batch:{}, n_head:{}, block:{}, head_embed:{} \".format(B, n_head, T, C//attention.n_head))\n",
    "\n",
    "\n",
    "print('---------------4.多头计算attention，直接使用torch function--------------')\n",
    "# 计算缩放点积注意力\n",
    "y = torch.nn.functional.scaled_dot_product_attention(q, k, v, \n",
    "                                                     attn_mask=None,\n",
    "                                                     dropout_p=attention.dropout if attention.training else 0, \n",
    "                                                     is_causal=True)\n",
    "print('y:', y.shape)\n",
    "\n",
    "\n",
    "print('---------------5.将多头注意力结果拼接--------------')\n",
    "# re-assemble all head outputs side by side\n",
    "y = y.transpose(1, 2).contiguous().view(B, T, C)  # transpose\n",
    "print('y-concat:', y.shape)\n",
    "\n",
    "\n",
    "print('---------------6. 增加一次前向传播--------------')\n",
    "# 将重塑后的输出通过自注意力层的另一个线性层，并应用残差dropout\n",
    "y = attention.resid_dropout(attention.c_proj(y))\n",
    "print(\"y_proj:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "masked_matrix = torch.tril(torch.ones(T, T)).view(1, 1, T, T)\n",
    "print(masked_matrix[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5行代码实现多头注意力计算\n",
      "q score: torch.Size([16, 12, 256, 64])\n",
      "k score: torch.Size([16, 12, 256, 64])\n",
      "k_t score: torch.Size([16, 12, 64, 256])\n",
      "q @ k_t score: torch.Size([16, 12, 256, 256])\n",
      "attn score: torch.Size([16, 12, 256, 256])\n",
      "attn score: torch.Size([16, 12, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "# attention.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "#                                         .view(1, 1, block_size, block_size))\n",
    "# print(bias)\n",
    "# 不使用torch2.0 attention计算\n",
    "print('5行代码实现多头注意力计算')\n",
    "# 1. scale and dot product process\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "print(\"q score:\", q.shape)\n",
    "print(\"k score:\", k.shape)\n",
    "print(\"k_t score:\", k.transpose(-2, -1).shape)\n",
    "print(\"q @ k_t score:\", (q@k.transpose(-2, -1)).shape)\n",
    "print(\"attn score:\", att.shape)\n",
    "# 2. Mask为下三角矩阵\n",
    "att = att.masked_fill(masked_matrix == 0, float('-inf'))\n",
    "# 3. softmax\n",
    "att = F.softmax(att, dim=-1)\n",
    "# 4. attn\n",
    "att = attention.attn_dropout(att)\n",
    "# 5. score\n",
    "y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "print(\"attn score:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(masked_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "x: torch.Size([16, 256, 768])\n",
      "x_fc: torch.Size([16, 256, 3072])\n",
      "x_proj: torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# mlp实现\n",
    "mlp = model.transformer.h[0].mlp\n",
    "print(mlp)\n",
    "print(\"x:\", x.shape)\n",
    "x = mlp.c_fc(x)\n",
    "print(\"x_fc:\", x.shape)\n",
    "x = mlp.gelu(x)\n",
    "x = mlp.c_proj(x)\n",
    "print(\"x_proj:\", x.shape)\n",
    "x = mlp.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 768])\n",
      "torch.Size([16, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# layer normalization\n",
    "ln = model.transformer.h[0].ln_1\n",
    "print(x.shape)\n",
    "F.layer_norm(x, ln.weight.shape, ln.weight, ln.bias, 1e-5)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "Mask = torch.tril(torch.ones(5, 5)).view(1, 1, 5, 5)\n",
    "print(Mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__sow', 'vi', 'z', 'zi', 'ni', '__eow', '__sow', ':', '__eow', 'he', 'didn', \"'\", 't', 'fall', '__sow', '?', '__eow', '__sow', 'in', 'co', 'n', 'ce', 'iv', 'ab', 'le', '__eow', '__sow', '!', '__eow']\n",
      "[24, 108, 82, 83, 71, 25, 24, 154, 25, 14, 10, 11, 12, 13, 24, 85, 25, 24, 140, 59, 39, 157, 87, 165, 114, 25, 24, 148, 25]\n",
      "vizzini : he didn ' t fall ? inconceivable !\n"
     ]
    }
   ],
   "source": [
    "from bpe import Encoder\n",
    "import sys\n",
    "sys.path.append('.')  # 将当前目录添加到Python的模块搜索路径。这样做是为了能够导入当前目录下的模块\n",
    "\n",
    "\n",
    "test_corpus = '''\n",
    "    Object raspberrypi functools dict kwargs. Gevent raspberrypi functools. Dunder raspberrypi decorator dict didn't lambda zip import pyramid, she lambda iterate?\n",
    "    Kwargs raspberrypi diversity unit object gevent. Import fall integration decorator unit django yield functools twisted. Dunder integration decorator he she future. Python raspberrypi community pypy. Kwargs integration beautiful test reduce gil python closure. Gevent he integration generator fall test kwargs raise didn't visor he itertools...\n",
    "    Reduce integration coroutine bdfl he python. Cython didn't integration while beautiful list python didn't nit!\n",
    "    Object fall diversity 2to3 dunder script. Python fall for: integration exception dict kwargs dunder pycon. Import raspberrypi beautiful test import six web. Future integration mercurial self script web. Return raspberrypi community test she stable.\n",
    "    Django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse. Dunder raspberrypi mercurial list reduce class test scipy helmet zip?\n",
    "'''\n",
    "\n",
    "# params chosen for demonstration purposes\n",
    "# 词汇表大小为200，其中88%（即176个）的词汇将由BPE生成，剩下的12%（即24个）的词汇将是单个字符。\n",
    "encoder = Encoder(200, pct_bpe=0.88)\n",
    "encoder.fit(test_corpus.split('\\n'))\n",
    "\n",
    "example = \"Vizzini: He didn't fall? INCONCEIVABLE!\"\n",
    "print(encoder.tokenize(example)) \n",
    "print(next(encoder.transform([example])))\n",
    "print(next(encoder.inverse_transform(encoder.transform([example]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__eow': 3, '__sow': 4, 'e': 5, 'r': 6, 'er': 7, 'f': 8, 'i': 9, 'n': 10, 's': 11, 't': 12, 'fi': 13, 'in': 14, 'ne': 15, 'es': 16, 'st': 17, 'o': 18, 'l': 19, 'd': 20, 'ol': 21, 'ld': 22, 'de': 23, 'b': 24, 'be': 25}\n",
      "['__sow', 'ol', 'de', 'st', '__eow']\n",
      "[4, 21, 23, 17, 3]\n",
      "oldest\n"
     ]
    }
   ],
   "source": [
    "from bpe import Encoder\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "test_corpus = '''\n",
    "    old older finest finer best \n",
    "'''\n",
    "\n",
    "encoder = Encoder(30, pct_bpe=0.88)  # params chosen for demonstration purposes\n",
    "encoder.fit(test_corpus.split('\\n'))\n",
    "print(encoder.bpe_vocab)\n",
    "\n",
    "example = \"oldest\"\n",
    "print(encoder.tokenize(example))\n",
    "print(next(encoder.transform([example])))\n",
    "print(next(encoder.inverse_transform(encoder.transform([example]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer)\n",
    "\n",
    "idx = tokenizer(\"pneumonoultramicroscopicsilicovolcanoconiosis\")[\"input_ids\"]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPT生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# NanoGPT\n",
    "def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测词表大小 65\n",
      "输入X: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1]])\n",
      "输入X长度: torch.Size([1, 10])\n",
      "输出Logits: torch.Size([1, 1, 65])\n",
      "输出Probs: torch.Size([1, 1, 65])\n",
      "tensor([[[0.0056, 0.0283, 0.0148, 0.0266, 0.0032, 0.0164, 0.0146, 0.0121,\n",
      "          0.0085, 0.0279, 0.0116, 0.0108, 0.0156, 0.0094, 0.0293, 0.0083,\n",
      "          0.0114, 0.0336, 0.0144, 0.0314, 0.0182, 0.0098, 0.0190, 0.0178,\n",
      "          0.0196, 0.0120, 0.0126, 0.0138, 0.0113, 0.0144, 0.0189, 0.0072,\n",
      "          0.0262, 0.0077, 0.0210, 0.0154, 0.0101, 0.0126, 0.0108, 0.0270,\n",
      "          0.0085, 0.0257, 0.0118, 0.0273, 0.0121, 0.0076, 0.0039, 0.0077,\n",
      "          0.0083, 0.0162, 0.0126, 0.0117, 0.0082, 0.0138, 0.0120, 0.0077,\n",
      "          0.0071, 0.0269, 0.0142, 0.0303, 0.0162, 0.0225, 0.0162, 0.0254,\n",
      "          0.0068]]], grad_fn=<SoftmaxBackward0>)\n",
      "输出下一个Token: torch.Size([1, 1])\n",
      "tensor([[17]])\n",
      "当前的token长度: 11\n",
      "当前的token序列: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 17]])\n",
      "输出Logits: torch.Size([1, 1, 65])\n",
      "输出Probs: torch.Size([1, 1, 65])\n",
      "tensor([[[0.0078, 0.0151, 0.0141, 0.0261, 0.0044, 0.0169, 0.0101, 0.0061,\n",
      "          0.0060, 0.0274, 0.0146, 0.0094, 0.0085, 0.0033, 0.0219, 0.0092,\n",
      "          0.0163, 0.0694, 0.0158, 0.0185, 0.0112, 0.0098, 0.0223, 0.0298,\n",
      "          0.0124, 0.0083, 0.0129, 0.0062, 0.0115, 0.0093, 0.0218, 0.0226,\n",
      "          0.0085, 0.0078, 0.0277, 0.0136, 0.0129, 0.0152, 0.0094, 0.0245,\n",
      "          0.0236, 0.0303, 0.0222, 0.0240, 0.0113, 0.0152, 0.0111, 0.0089,\n",
      "          0.0191, 0.0122, 0.0084, 0.0059, 0.0166, 0.0099, 0.0090, 0.0149,\n",
      "          0.0069, 0.0355, 0.0099, 0.0161, 0.0151, 0.0133, 0.0196, 0.0164,\n",
      "          0.0058]]], grad_fn=<SoftmaxBackward0>)\n",
      "输出下一个Token: torch.Size([1, 1])\n",
      "tensor([[17]])\n",
      "当前的token长度: 12\n",
      "当前的token序列: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 17, 17]])\n"
     ]
    }
   ],
   "source": [
    "# Greedy Generation\n",
    "# 词表大小为65，每次预测都是65个概率值，表示下一个token的概率，取概率最大的token作为下一个token\n",
    "idx = X[1, :10].reshape(1, 10)  # generate 1\n",
    "print(\"预测词表大小\", model.config.vocab_size)  # BPE词表\n",
    "print(\"输入X:\", idx)\n",
    "print(\"输入X长度:\", idx.shape)\n",
    "for _ in range(2):\n",
    "    logits, _ = model(idx)\n",
    "    print('输出Logits:', logits.shape)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print('输出Probs:', probs.shape)\n",
    "    print(probs)\n",
    "\n",
    "    idx_next = torch.argmax(probs, dim=2)  # dim=2表示在第2维度上取最大值\n",
    "    print('输出下一个Token:', idx_next.shape)\n",
    "    print(idx_next)\n",
    "\n",
    "    idx = torch.cat((idx, idx_next), dim=1)  # 将下一个Token拼接到输入序列上\n",
    "    print(\"当前的token长度:\", len(idx[0]))\n",
    "    print(\"当前的token序列:\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4)  # 设置打印精度\n",
    "torch.set_printoptions(sci_mode=False)  # 设置打印模式为非科学计数法\n",
    "torch.set_printoptions(linewidth=100)  # 设置打印行宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "65\n",
      "prompt: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1]])\n",
      "idx_cond: torch.Size([1, 10])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-0.8807,  0.3733,  0.1661,  0.6737, -1.1515, -0.0462,  0.3196, -0.7420, -0.7176,  0.6705,\n",
      "         -0.4549, -0.0960, -0.6452, -0.6828, -0.4365, -0.0968, -0.8049,  0.8895, -0.1366,  0.1941,\n",
      "          1.0271,  0.0642,  0.5817,  0.4299,  0.1736,  0.0942,  0.0415, -0.1390, -0.1196,  0.0969,\n",
      "         -0.0389, -0.1811,  0.4824, -1.5111,  0.6647,  0.2604, -0.6491, -0.2169, -0.8141,  0.1885,\n",
      "         -0.4787,  0.3125, -0.2149,  0.7776,  0.0711, -0.2495, -0.4972, -0.6088, -0.4011, -0.0208,\n",
      "         -0.2463, -0.1300, -1.1869, -0.6447, -0.4758, -0.7341, -0.0147,  0.7913, -0.7784,  0.0524,\n",
      "          0.4630,  0.3149,  0.0990,  0.1935, -0.3702]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0062, 0.0217, 0.0177, 0.0294, 0.0047, 0.0143, 0.0206, 0.0071, 0.0073, 0.0293, 0.0095,\n",
      "         0.0136, 0.0079, 0.0076, 0.0097, 0.0136, 0.0067, 0.0364, 0.0131, 0.0182, 0.0418, 0.0160,\n",
      "         0.0268, 0.0230, 0.0178, 0.0165, 0.0156, 0.0130, 0.0133, 0.0165, 0.0144, 0.0125, 0.0243,\n",
      "         0.0033, 0.0291, 0.0194, 0.0078, 0.0121, 0.0066, 0.0181, 0.0093, 0.0205, 0.0121, 0.0326,\n",
      "         0.0161, 0.0117, 0.0091, 0.0081, 0.0100, 0.0147, 0.0117, 0.0131, 0.0046, 0.0079, 0.0093,\n",
      "         0.0072, 0.0148, 0.0330, 0.0069, 0.0158, 0.0238, 0.0205, 0.0165, 0.0182, 0.0103]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[59]])\n",
      "torch.Size([1, 11])\n",
      "generate: length : 11\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59]])\n",
      "idx_cond: torch.Size([1, 11])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-1.3232, -0.0773,  0.2743,  0.3964, -1.3851,  0.2521,  0.0582, -0.4030, -0.5447,  0.8651,\n",
      "          0.4530, -0.8679, -0.3778, -0.9318, -0.1542, -0.0103,  0.4552,  0.8329,  0.0847,  0.6118,\n",
      "         -0.4826, -0.0063,  0.4400,  0.4722,  0.6509, -0.0239,  0.1730, -0.4748,  0.0264,  0.0687,\n",
      "          0.0315,  0.2866, -0.5111, -0.5414,  0.3006,  0.1780,  0.0538,  0.2655, -0.5289,  0.9260,\n",
      "          0.4371,  0.6746, -0.7930,  0.7723,  0.5907,  0.0116, -0.9404,  0.1466, -0.4184,  0.6245,\n",
      "         -0.6404, -0.2432, -0.6395,  0.2081, -0.4905, -0.7910, -0.3595,  0.6770, -0.4075,  0.7324,\n",
      "         -0.3772, -0.1873,  0.5270, -0.1306, -0.5268]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0037, 0.0128, 0.0182, 0.0206, 0.0035, 0.0178, 0.0147, 0.0092, 0.0080, 0.0328, 0.0218,\n",
      "         0.0058, 0.0095, 0.0054, 0.0119, 0.0137, 0.0218, 0.0318, 0.0150, 0.0255, 0.0085, 0.0137,\n",
      "         0.0215, 0.0222, 0.0265, 0.0135, 0.0164, 0.0086, 0.0142, 0.0148, 0.0143, 0.0184, 0.0083,\n",
      "         0.0080, 0.0187, 0.0165, 0.0146, 0.0180, 0.0081, 0.0349, 0.0214, 0.0271, 0.0063, 0.0299,\n",
      "         0.0250, 0.0140, 0.0054, 0.0160, 0.0091, 0.0258, 0.0073, 0.0108, 0.0073, 0.0170, 0.0085,\n",
      "         0.0063, 0.0097, 0.0272, 0.0092, 0.0288, 0.0095, 0.0115, 0.0234, 0.0121, 0.0082]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[28]])\n",
      "torch.Size([1, 12])\n",
      "generate: length : 12\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28]])\n",
      "idx_cond: torch.Size([1, 12])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-6.2877e-01, -4.7042e-01,  5.4306e-01,  1.0810e-01, -6.3371e-01, -3.2131e-01, -4.9273e-01,\n",
      "          9.2538e-02,  4.1309e-01,  2.0295e-01,  2.9460e-01, -7.9125e-01, -6.5641e-01,  4.1444e-02,\n",
      "          3.2030e-01, -1.8466e-01, -7.5907e-01,  1.1130e+00, -4.7979e-01,  3.5508e-01,  6.0116e-01,\n",
      "         -4.5124e-01, -4.7121e-01,  4.7233e-01,  4.8277e-01, -2.4493e-01,  5.2312e-02,  2.1881e-01,\n",
      "          5.6344e-01,  3.8853e-01,  4.1296e-01, -1.5221e-01, -2.1972e-01, -6.8657e-01, -3.4652e-01,\n",
      "         -5.1094e-01, -5.6706e-03,  8.0922e-01, -4.3093e-01,  1.2517e-01,  4.1629e-01,  5.3373e-01,\n",
      "         -1.4380e+00, -1.7897e-01,  6.1275e-02, -1.7157e-01, -1.1321e+00, -4.5396e-01, -1.5056e-01,\n",
      "          1.2101e-01, -7.1800e-04,  5.4164e-01, -7.1662e-01,  1.5484e-01, -9.1801e-01, -9.3972e-01,\n",
      "         -3.3918e-01,  5.1516e-02, -2.3265e-01,  7.4550e-01,  3.7502e-02,  2.1901e-01,  2.6833e-01,\n",
      "         -2.7699e-01, -1.6120e+00]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0080, 0.0093, 0.0257, 0.0166, 0.0079, 0.0108, 0.0091, 0.0164, 0.0226, 0.0183, 0.0200,\n",
      "         0.0068, 0.0077, 0.0156, 0.0206, 0.0124, 0.0070, 0.0454, 0.0092, 0.0213, 0.0272, 0.0095,\n",
      "         0.0093, 0.0239, 0.0242, 0.0117, 0.0157, 0.0186, 0.0262, 0.0220, 0.0226, 0.0128, 0.0120,\n",
      "         0.0075, 0.0106, 0.0090, 0.0148, 0.0335, 0.0097, 0.0169, 0.0226, 0.0255, 0.0035, 0.0125,\n",
      "         0.0159, 0.0126, 0.0048, 0.0095, 0.0128, 0.0169, 0.0149, 0.0257, 0.0073, 0.0174, 0.0060,\n",
      "         0.0058, 0.0106, 0.0157, 0.0118, 0.0315, 0.0155, 0.0186, 0.0195, 0.0113, 0.0030]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[17]])\n",
      "torch.Size([1, 13])\n",
      "generate: length : 13\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17]])\n",
      "idx_cond: torch.Size([1, 13])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-0.3055, -0.1471,  0.3005,  0.0784, -0.1422,  0.2211, -0.1861, -0.2322,  0.0786,  0.4725,\n",
      "          0.2662, -0.8145,  0.2767,  0.1183,  0.5535, -0.2904, -0.4245,  1.7450, -0.4461,  0.6851,\n",
      "          0.1180, -0.3091, -0.0641,  0.7902,  0.0181,  0.0220,  0.3453, -0.2790, -0.3321, -0.0421,\n",
      "          0.7473, -0.9089, -0.4361, -0.5279,  0.9439, -0.6655,  0.8484,  0.3132, -0.1127,  0.5644,\n",
      "         -0.1317,  0.3689, -1.2771,  0.8058,  0.0216, -0.8428, -0.3726, -0.8412, -0.1425, -0.2144,\n",
      "         -0.0958, -0.2700, -0.2181, -0.4902, -0.2885, -0.4884, -0.4805, -0.3443, -1.0961,  0.5835,\n",
      "         -0.4716,  0.7693, -0.3405,  0.6146, -1.0944]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0101, 0.0118, 0.0185, 0.0148, 0.0119, 0.0171, 0.0114, 0.0109, 0.0148, 0.0220, 0.0179,\n",
      "         0.0061, 0.0181, 0.0154, 0.0239, 0.0103, 0.0090, 0.0786, 0.0088, 0.0272, 0.0154, 0.0101,\n",
      "         0.0129, 0.0302, 0.0140, 0.0140, 0.0194, 0.0104, 0.0098, 0.0132, 0.0290, 0.0055, 0.0089,\n",
      "         0.0081, 0.0353, 0.0071, 0.0320, 0.0188, 0.0123, 0.0241, 0.0120, 0.0198, 0.0038, 0.0307,\n",
      "         0.0140, 0.0059, 0.0095, 0.0059, 0.0119, 0.0111, 0.0125, 0.0105, 0.0110, 0.0084, 0.0103,\n",
      "         0.0084, 0.0085, 0.0097, 0.0046, 0.0246, 0.0086, 0.0296, 0.0098, 0.0254, 0.0046]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[53]])\n",
      "torch.Size([1, 14])\n",
      "generate: length : 14\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17, 53]])\n",
      "idx_cond: torch.Size([1, 14])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-0.8571, -0.3470,  0.2121,  0.4653, -0.6405,  0.1884, -0.2205,  0.5455, -0.4972,  1.1885,\n",
      "          0.1783, -0.2423, -0.7720,  0.1007,  0.5176,  0.0215, -0.8983,  0.6070, -0.6195,  0.6354,\n",
      "         -0.0128, -0.5004,  0.5337,  0.6529, -0.2180, -0.3667, -0.3982,  0.2399, -0.4899,  0.3925,\n",
      "          0.8446,  0.0081,  1.0931, -0.5403,  0.7800,  0.2278,  0.4078,  0.9954, -0.3955,  0.4699,\n",
      "          0.4320,  0.8812, -0.6624,  0.8304, -0.3460, -0.4656, -1.1016, -0.8206, -0.4834,  0.6857,\n",
      "         -0.1197, -0.0652, -0.7632,  0.9441, -0.7344, -0.4424, -0.1581,  0.5535, -0.5228, -0.0525,\n",
      "         -0.7390, -0.1351, -0.8364, -0.5150, -1.2586]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0057, 0.0094, 0.0165, 0.0213, 0.0070, 0.0161, 0.0107, 0.0231, 0.0081, 0.0438, 0.0160,\n",
      "         0.0105, 0.0062, 0.0148, 0.0224, 0.0136, 0.0054, 0.0245, 0.0072, 0.0252, 0.0132, 0.0081,\n",
      "         0.0228, 0.0257, 0.0107, 0.0093, 0.0090, 0.0170, 0.0082, 0.0198, 0.0311, 0.0135, 0.0399,\n",
      "         0.0078, 0.0291, 0.0168, 0.0201, 0.0361, 0.0090, 0.0214, 0.0206, 0.0322, 0.0069, 0.0306,\n",
      "         0.0095, 0.0084, 0.0044, 0.0059, 0.0082, 0.0265, 0.0119, 0.0125, 0.0062, 0.0343, 0.0064,\n",
      "         0.0086, 0.0114, 0.0232, 0.0079, 0.0127, 0.0064, 0.0117, 0.0058, 0.0080, 0.0038]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[19]])\n",
      "torch.Size([1, 15])\n",
      "generate: length : 15\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17, 53, 19]])\n",
      "idx_cond: torch.Size([1, 15])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-0.4870, -1.1776,  1.0129,  0.6206,  0.1232,  1.0627,  0.0807,  0.2122, -0.1878,  0.5017,\n",
      "          0.2253, -0.6050, -0.6967, -0.1605,  0.4229, -0.8251, -1.2501,  0.2592,  0.0457,  0.8944,\n",
      "          0.3504, -0.3832, -0.5295,  0.3728, -0.2871, -0.1049, -0.4102,  0.0063, -0.9955, -0.0836,\n",
      "         -0.0768,  0.2669, -0.7295, -0.5914,  0.3979, -0.5234,  0.1582,  0.2054, -0.1257, -0.0119,\n",
      "          0.5010,  0.6546, -1.4574,  0.3587, -0.0675, -0.1786, -1.2367, -0.0908,  0.0688,  0.5905,\n",
      "         -0.1369,  0.5370, -1.4599, -0.1545, -0.2593, -0.3325, -0.5986, -0.1545, -0.8243,  0.2389,\n",
      "          0.2921,  0.6671,  0.0245,  0.2440, -1.3948]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0090, 0.0045, 0.0404, 0.0273, 0.0166, 0.0425, 0.0159, 0.0181, 0.0122, 0.0242, 0.0184,\n",
      "         0.0080, 0.0073, 0.0125, 0.0224, 0.0064, 0.0042, 0.0190, 0.0154, 0.0359, 0.0208, 0.0100,\n",
      "         0.0086, 0.0213, 0.0110, 0.0132, 0.0097, 0.0148, 0.0054, 0.0135, 0.0136, 0.0192, 0.0071,\n",
      "         0.0081, 0.0219, 0.0087, 0.0172, 0.0180, 0.0129, 0.0145, 0.0242, 0.0282, 0.0034, 0.0210,\n",
      "         0.0137, 0.0123, 0.0043, 0.0134, 0.0157, 0.0265, 0.0128, 0.0251, 0.0034, 0.0126, 0.0113,\n",
      "         0.0105, 0.0081, 0.0126, 0.0064, 0.0186, 0.0197, 0.0286, 0.0150, 0.0187, 0.0036]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[60]])\n",
      "torch.Size([1, 16])\n",
      "generate: length : 16\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17, 53, 19, 60]])\n",
      "idx_cond: torch.Size([1, 16])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-7.8059e-01, -3.5383e-01,  3.5880e-01,  3.8600e-01, -5.3034e-01,  5.1261e-01,  3.7773e-01,\n",
      "         -1.0323e+00, -8.4659e-01,  4.3531e-01, -2.6502e-01, -7.3522e-01, -4.8492e-01, -4.9937e-01,\n",
      "          1.5144e-01, -3.4754e-01, -1.0534e-01,  9.4930e-01,  4.9711e-01, -2.3422e-01,  3.3390e-01,\n",
      "         -3.2118e-01,  5.4899e-01,  6.9615e-01, -6.8202e-01, -2.2893e-01, -4.5288e-02,  1.6822e-01,\n",
      "          4.6654e-01,  4.6546e-01,  3.2424e-01,  6.0931e-01, -2.5247e-01, -1.2854e+00,  8.3977e-01,\n",
      "         -8.0581e-02,  1.1877e-02, -1.5536e-01, -8.2585e-01,  5.6254e-01,  2.0453e-01,  1.0780e+00,\n",
      "          2.0332e-01, -1.1610e-01,  5.3320e-01,  3.6062e-01, -1.2677e+00, -3.1203e-01, -1.7467e-01,\n",
      "          7.9070e-01, -5.6541e-04, -1.1071e+00, -6.1715e-01, -8.5481e-02, -4.3000e-01, -8.5304e-01,\n",
      "          4.3899e-01, -4.0560e-01,  2.6411e-02,  2.6997e-01,  8.4626e-01,  3.3839e-01, -9.9272e-02,\n",
      "         -1.3471e-02, -1.2339e+00]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0063, 0.0097, 0.0198, 0.0204, 0.0081, 0.0231, 0.0202, 0.0049, 0.0059, 0.0214, 0.0106,\n",
      "         0.0066, 0.0085, 0.0084, 0.0161, 0.0098, 0.0125, 0.0358, 0.0228, 0.0110, 0.0193, 0.0100,\n",
      "         0.0240, 0.0278, 0.0070, 0.0110, 0.0132, 0.0164, 0.0221, 0.0220, 0.0191, 0.0255, 0.0108,\n",
      "         0.0038, 0.0321, 0.0128, 0.0140, 0.0119, 0.0061, 0.0243, 0.0170, 0.0407, 0.0170, 0.0123,\n",
      "         0.0236, 0.0199, 0.0039, 0.0101, 0.0116, 0.0305, 0.0138, 0.0046, 0.0075, 0.0127, 0.0090,\n",
      "         0.0059, 0.0215, 0.0092, 0.0142, 0.0181, 0.0323, 0.0194, 0.0125, 0.0137, 0.0040]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[30]])\n",
      "torch.Size([1, 17])\n",
      "generate: length : 17\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17, 53, 19, 60, 30]])\n",
      "idx_cond: torch.Size([1, 17])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-0.8212,  0.1596,  0.0299,  0.1656, -1.0485,  0.3322, -0.0910,  0.2612, -0.3517,  0.5113,\n",
      "          0.1172,  0.0689, -0.6264, -0.5266,  0.1342, -0.4234, -0.0094,  0.8003, -0.5034,  1.0292,\n",
      "          0.2307, -0.4623,  0.1416,  0.6665,  0.3166, -1.3126, -0.2819, -0.1232,  0.1498,  0.8678,\n",
      "          2.1881, -0.2867,  0.4819,  0.1313,  0.5711,  0.4210,  0.5157,  0.7852,  0.5393,  0.2004,\n",
      "         -0.5401,  0.9318, -0.8792,  0.7472, -0.0159, -0.1358,  0.1137, -0.7460, -0.7217, -0.0844,\n",
      "          0.3007, -0.0427, -0.2834, -0.0481, -0.3561, -0.2812, -0.8082,  0.4559, -0.5865, -0.2155,\n",
      "          0.5220, -0.4625,  0.4901,  0.5542, -1.0105]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0054, 0.0144, 0.0126, 0.0145, 0.0043, 0.0171, 0.0112, 0.0159, 0.0086, 0.0204, 0.0138,\n",
      "         0.0131, 0.0065, 0.0072, 0.0140, 0.0080, 0.0121, 0.0273, 0.0074, 0.0343, 0.0154, 0.0077,\n",
      "         0.0141, 0.0239, 0.0168, 0.0033, 0.0092, 0.0108, 0.0142, 0.0292, 0.1092, 0.0092, 0.0198,\n",
      "         0.0140, 0.0217, 0.0187, 0.0205, 0.0269, 0.0210, 0.0150, 0.0071, 0.0311, 0.0051, 0.0259,\n",
      "         0.0121, 0.0107, 0.0137, 0.0058, 0.0060, 0.0113, 0.0165, 0.0117, 0.0092, 0.0117, 0.0086,\n",
      "         0.0092, 0.0055, 0.0193, 0.0068, 0.0099, 0.0206, 0.0077, 0.0200, 0.0213, 0.0045]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[59]])\n",
      "torch.Size([1, 18])\n",
      "generate: length : 18\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17, 53, 19, 60, 30, 59]])\n",
      "idx_cond: torch.Size([1, 18])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-0.9099, -0.8633, -0.0671,  0.0477, -0.9604,  0.9836, -0.5391,  0.0831, -0.0885,  0.6634,\n",
      "          0.3099, -0.3923, -0.3453, -0.9189,  0.3847,  0.4697, -1.4315,  0.9571, -0.8072,  0.1801,\n",
      "         -0.4747, -0.4185,  0.2018,  0.1208,  0.1360, -0.2506,  0.3503, -0.9093, -0.2512,  0.3106,\n",
      "          0.1364,  0.8538,  0.2088, -0.3689,  0.1200,  0.0995,  0.5843,  0.3052,  0.5380,  1.0569,\n",
      "         -0.1345,  1.1026, -0.7539,  0.4127,  0.2143,  0.4499, -0.7325, -0.7670, -1.1036,  0.4210,\n",
      "         -0.3247,  0.0027, -0.7713, -0.1246, -0.3164, -0.3564, -0.1934, -0.4445, -0.4618,  1.1924,\n",
      "         -0.7874,  0.0910,  0.3734,  0.2452, -0.6736]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0056, 0.0058, 0.0130, 0.0145, 0.0053, 0.0371, 0.0081, 0.0151, 0.0127, 0.0269, 0.0189,\n",
      "         0.0094, 0.0098, 0.0055, 0.0204, 0.0222, 0.0033, 0.0361, 0.0062, 0.0166, 0.0086, 0.0091,\n",
      "         0.0170, 0.0156, 0.0159, 0.0108, 0.0197, 0.0056, 0.0108, 0.0189, 0.0159, 0.0325, 0.0171,\n",
      "         0.0096, 0.0156, 0.0153, 0.0249, 0.0188, 0.0237, 0.0399, 0.0121, 0.0417, 0.0065, 0.0209,\n",
      "         0.0172, 0.0217, 0.0067, 0.0064, 0.0046, 0.0211, 0.0100, 0.0139, 0.0064, 0.0122, 0.0101,\n",
      "         0.0097, 0.0114, 0.0089, 0.0087, 0.0457, 0.0063, 0.0152, 0.0201, 0.0177, 0.0071]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[8]])\n",
      "torch.Size([1, 19])\n",
      "generate: length : 19\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17, 53, 19, 60, 30, 59,  8]])\n",
      "idx_cond: torch.Size([1, 19])\n",
      "logits: torch.Size([1, 1, 65])\n",
      "no - 1 torch.Size([1, 65])\n",
      "logits: torch.Size([1, 65])\n",
      "top_k v: torch.Size([1, 5])\n",
      "top_k logits: torch.Size([1, 65])\n",
      "tensor([[-1.0878, -0.3114,  0.2579, -1.4351, -0.3420, -0.0997, -0.1523, -0.2620,  0.7557, -0.1927,\n",
      "          0.3994, -0.2045, -1.0847, -0.5053,  0.1566,  0.0362,  0.4817,  0.8107, -0.6901,  0.2935,\n",
      "          0.3293, -0.4596, -0.6625,  0.8451, -0.0878,  0.0860,  0.0519, -0.5300, -0.3641, -0.2249,\n",
      "          0.0349,  0.3957,  0.3878, -0.0403,  0.2447, -0.7876, -0.2406,  0.0375,  0.4904,  0.3868,\n",
      "         -0.0169, -0.2274, -1.7098,  0.2037,  0.0818,  0.0041, -1.1083, -0.7964, -0.7378,  0.6390,\n",
      "          0.1499, -0.4770, -0.9335,  0.2576,  0.1474, -1.5209,  0.1740, -0.4240, -0.6968,  0.8157,\n",
      "          0.2606, -0.0711,  0.6592,  1.1495, -0.6514]], grad_fn=<DivBackward0>)\n",
      "probs : torch.Size([1, 65])\n",
      "tensor([[0.0050, 0.0109, 0.0192, 0.0035, 0.0106, 0.0134, 0.0128, 0.0114, 0.0316, 0.0123, 0.0222,\n",
      "         0.0121, 0.0050, 0.0090, 0.0174, 0.0154, 0.0241, 0.0334, 0.0075, 0.0199, 0.0207, 0.0094,\n",
      "         0.0077, 0.0346, 0.0136, 0.0162, 0.0156, 0.0087, 0.0103, 0.0119, 0.0154, 0.0221, 0.0219,\n",
      "         0.0143, 0.0190, 0.0068, 0.0117, 0.0154, 0.0243, 0.0219, 0.0146, 0.0118, 0.0027, 0.0182,\n",
      "         0.0161, 0.0149, 0.0049, 0.0067, 0.0071, 0.0281, 0.0173, 0.0092, 0.0058, 0.0192, 0.0172,\n",
      "         0.0032, 0.0177, 0.0097, 0.0074, 0.0336, 0.0193, 0.0138, 0.0287, 0.0469, 0.0077]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "idx_next : torch.Size([1, 1])\n",
      "tensor([[63]])\n",
      "torch.Size([1, 20])\n",
      "generate: length : 20\n",
      "generate: tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1, 59, 28, 17, 53, 19, 60, 30, 59,  8, 63]])\n"
     ]
    }
   ],
   "source": [
    "# model.generate()\n",
    "# top-k GPT-2\n",
    "idx = X[1, :10].reshape(1, 10)  # generate 1\n",
    "print(idx.shape)\n",
    "temperature = 1.0\n",
    "top_k = 5\n",
    "print(model.config.vocab_size)  # BPE词表\n",
    "print(\"prompt:\", idx)\n",
    "\n",
    "# top k的方法\n",
    "# 词表 65\n",
    "# llama 32000\n",
    "\n",
    "for _ in range(10):\n",
    "    idx_cond = idx if idx.size(\n",
    "        1) <= model.config.block_size else idx[:, -model.config.block_size:]\n",
    "    print('idx_cond:', idx_cond.shape)\n",
    "\n",
    "    logits, _ = model(idx_cond)\n",
    "    print('logits:', logits.shape)\n",
    "\n",
    "    print(\"no - 1\", logits[:, -1, :].shape)  # 为什么要-1来降维\n",
    "\n",
    "    logits = logits[:, -1, :] / temperature  # 平缓， tips : 知识蒸馏[温度]\n",
    "    print('logits:', logits.shape)\n",
    "\n",
    "    if top_k is not None:\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        print('top_k v:', v.shape)\n",
    "\n",
    "#         logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        print('top_k logits:', logits.shape)\n",
    "        print(logits)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print('probs :', probs.shape)\n",
    "    print(probs)\n",
    "\n",
    "    # num_samples-by-num_samples\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)  # 从多项分布中采样\n",
    "    print('idx_next :', idx_next.shape)\n",
    "    print(idx_next)\n",
    "\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "    print(idx.shape)\n",
    "\n",
    "    print(\"generate: length :\", len(idx[0]))\n",
    "    print(\"generate:\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate(idx, 20, 1.0, None) # temparature 1.0 ,  Top_K None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.3000, 0.3000, 0.0000])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为1: 冬\n",
      "第1次sample的next_token下标为0: 小\n",
      "第2次sample的next_token下标为0: 小\n",
      "第3次sample的next_token下标为1: 冬\n",
      "第4次sample的next_token下标为0: 小\n",
      "第5次sample的next_token下标为0: 小\n",
      "第6次sample的next_token下标为0: 小\n",
      "第7次sample的next_token下标为1: 冬\n",
      "第8次sample的next_token下标为1: 冬\n",
      "第9次sample的next_token下标为0: 小\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "dict_map = {0: \"小\", 1: \"冬\", 2: \"瓜\"}\n",
    "prob = torch.tensor([0.3, 0.3, 0.0])\n",
    "print(\"prob : \", prob)\n",
    "\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(10):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\",\n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.7000, 0.2000, 0.1000])\n",
      "prob/T :  tensor([0.3500, 0.1000, 0.0500])\n",
      "softmax(prob/T) :  tensor([0.3969, 0.3091, 0.2940])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为1: 冬\n",
      "第1次sample的next_token下标为0: 小\n",
      "第2次sample的next_token下标为1: 冬\n",
      "第3次sample的next_token下标为0: 小\n",
      "第4次sample的next_token下标为1: 冬\n",
      "第5次sample的next_token下标为0: 小\n",
      "第6次sample的next_token下标为0: 小\n",
      "第7次sample的next_token下标为1: 冬\n",
      "第8次sample的next_token下标为1: 冬\n",
      "第9次sample的next_token下标为1: 冬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/pzgqqgv92qd3p3t4m01dhbp00000gn/T/ipykernel_84500/4166687864.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(prob)\n"
     ]
    }
   ],
   "source": [
    "# Do Sample with temparature\n",
    "temparature = 2.0\n",
    "dict_map = {0: \"小\", 1: \"冬\", 2: \"瓜\"}\n",
    "prob = torch.tensor([0.7, 0.2, 0.1])\n",
    "print(\"prob : \", prob)\n",
    "prob /= temparature\n",
    "print(\"prob/T : \", prob)\n",
    "prob = F.softmax(prob)\n",
    "print(\"softmax(prob/T) : \", prob)\n",
    "\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(10):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\",\n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.7000, 0.2000, 0.1000])\n",
      "prob/T :  tensor([0.3500, 0.1000, 0.0500])\n",
      "softmax(prob/T) :  tensor([0.3969, 0.3091, 0.2940])\n",
      "top-k: tensor([0.3969, 0.3091])\n",
      "top-k softmax: tensor([0.5219, 0.4781])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为0: 小\n",
      "第1次sample的next_token下标为0: 小\n",
      "第2次sample的next_token下标为0: 小\n",
      "第3次sample的next_token下标为1: 冬\n",
      "第4次sample的next_token下标为0: 小\n",
      "第5次sample的next_token下标为1: 冬\n",
      "第6次sample的next_token下标为1: 冬\n",
      "第7次sample的next_token下标为1: 冬\n",
      "第8次sample的next_token下标为0: 小\n",
      "第9次sample的next_token下标为0: 小\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/pzgqqgv92qd3p3t4m01dhbp00000gn/T/ipykernel_84500/2623233868.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(prob)\n",
      "/var/folders/5y/pzgqqgv92qd3p3t4m01dhbp00000gn/T/ipykernel_84500/2623233868.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(prob)\n"
     ]
    }
   ],
   "source": [
    "# top-k\n",
    "temparature = 2.0\n",
    "top_k = 2\n",
    "dict_map = {0: \"小\", 1: \"冬\", 2: \"瓜\"}\n",
    "prob = torch.tensor([0.7, 0.2, 0.1])\n",
    "print(\"prob : \", prob)\n",
    "prob /= temparature\n",
    "print(\"prob/T : \", prob)\n",
    "prob = F.softmax(prob)\n",
    "print(\"softmax(prob/T) : \", prob)\n",
    "prob, _ = torch.topk(prob, top_k)\n",
    "print(\"top-k:\", prob)\n",
    "prob = F.softmax(prob)\n",
    "print(\"top-k softmax:\", prob)\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(10):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\",\n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 13, 52, 42,  1, 26, 43, 56, 53,  1]])\n",
      "tensor([[-1.1206,  0.7769,  0.2215,  0.3090, -1.6671,  0.4123,  0.1556,  0.4466,  0.1015,  0.9674,\n",
      "         -0.3920, -0.3790, -0.5708, -0.6095,  0.0887, -0.6818, -1.1605,  0.5678, -0.3892,  0.6791,\n",
      "          0.1930, -0.0099,  0.7034,  0.4553,  0.0086,  0.1226, -0.3700,  0.1384, -0.2160,  0.0735,\n",
      "          0.6738,  0.3342,  0.1935, -1.0596,  0.3474,  0.3951, -0.4422,  0.2010, -0.2527,  0.5982,\n",
      "          0.5057,  0.0167, -0.4491,  0.5445,  0.7778, -0.1056, -1.1756, -0.5428, -0.7051,  0.1678,\n",
      "          0.0762, -0.2580, -1.0917,  0.3266, -0.2051, -0.0857, -1.0482,  0.7362, -0.1279,  0.3829,\n",
      "          0.5765,  0.1140,  0.2138,  0.6706, -0.5340]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-2.2412,  0.4837,  0.2215,  0.3090, -1.6671,  0.4123,  0.1556,  0.4466,  0.1015,  0.9674,\n",
      "         -0.3920, -0.3790, -0.5708,  0.3884,  0.0887, -0.6818, -1.1605,  0.5678, -0.3892,  0.6791,\n",
      "          0.1930, -0.0099,  0.7034,  0.4553,  0.0086,  0.1226,  0.2061,  0.1384, -0.2160,  0.0735,\n",
      "          0.6738,  0.3342,  0.1935, -1.0596,  0.3474,  0.3951, -0.4422,  0.2010, -0.2527,  0.5982,\n",
      "          0.5057,  0.0167,  0.1545,  0.0778,  0.7778, -0.1056, -1.1756, -0.5428, -0.7051,  0.1678,\n",
      "          0.0762, -0.2580,  0.1108,  0.0508, -0.2051, -0.0857,  0.2233,  0.7362, -0.1279,  0.3829,\n",
      "          0.5765,  0.1140,  0.2138,  0.6706, -0.5340]], grad_fn=<AsStridedBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# repetition penalty\n",
    "# GPT-2模型进行文本生成的过程，并在生成过程中应用了重复性惩罚\n",
    "idx = X[1, :10].reshape(1, 10)\n",
    "print(idx)\n",
    "penalty = 2.0  # 重复性惩罚的权重\n",
    "for _ in range(256):\n",
    "    logits, _ = model(idx)\n",
    "    logits = logits[:, -1, :]\n",
    "    original_logits = logits.clone()\n",
    "\n",
    "    print(logits)\n",
    "\n",
    "    # repetition penalty\n",
    "    logits_idx = torch.gather(logits, 1, idx)\n",
    "    logits_idx = torch.where(logits < 0, logits *\n",
    "                             penalty, logits / penalty).clone()\n",
    "    logits.scatter_(1, idx, logits_idx)\n",
    "    print(logits)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob :  tensor([0.3000, 0.3000, 0.0000])\n",
      "根据概率权重所选择next token的下标\n",
      "第0次sample的next_token下标为1: 冬\n",
      "第1次sample的next_token下标为1: 冬\n",
      "第2次sample的next_token下标为1: 冬\n",
      "第3次sample的next_token下标为0: 小\n",
      "第4次sample的next_token下标为1: 冬\n",
      "第5次sample的next_token下标为1: 冬\n",
      "第6次sample的next_token下标为0: 小\n",
      "第7次sample的next_token下标为1: 冬\n",
      "第8次sample的next_token下标为1: 冬\n",
      "第9次sample的next_token下标为1: 冬\n",
      "第10次sample的next_token下标为0: 小\n",
      "第11次sample的next_token下标为0: 小\n",
      "第12次sample的next_token下标为0: 小\n",
      "第13次sample的next_token下标为1: 冬\n",
      "第14次sample的next_token下标为0: 小\n",
      "第15次sample的next_token下标为1: 冬\n",
      "第16次sample的next_token下标为0: 小\n",
      "第17次sample的next_token下标为0: 小\n",
      "第18次sample的next_token下标为0: 小\n",
      "第19次sample的next_token下标为0: 小\n",
      "第20次sample的next_token下标为0: 小\n",
      "第21次sample的next_token下标为1: 冬\n",
      "第22次sample的next_token下标为1: 冬\n",
      "第23次sample的next_token下标为1: 冬\n",
      "第24次sample的next_token下标为1: 冬\n",
      "第25次sample的next_token下标为1: 冬\n",
      "第26次sample的next_token下标为0: 小\n",
      "第27次sample的next_token下标为0: 小\n",
      "第28次sample的next_token下标为1: 冬\n",
      "第29次sample的next_token下标为1: 冬\n",
      "第30次sample的next_token下标为0: 小\n",
      "第31次sample的next_token下标为1: 冬\n",
      "第32次sample的next_token下标为1: 冬\n",
      "第33次sample的next_token下标为1: 冬\n",
      "第34次sample的next_token下标为1: 冬\n",
      "第35次sample的next_token下标为1: 冬\n",
      "第36次sample的next_token下标为1: 冬\n",
      "第37次sample的next_token下标为1: 冬\n",
      "第38次sample的next_token下标为1: 冬\n",
      "第39次sample的next_token下标为1: 冬\n",
      "第40次sample的next_token下标为0: 小\n",
      "第41次sample的next_token下标为1: 冬\n",
      "第42次sample的next_token下标为1: 冬\n",
      "第43次sample的next_token下标为1: 冬\n",
      "第44次sample的next_token下标为1: 冬\n",
      "第45次sample的next_token下标为1: 冬\n",
      "第46次sample的next_token下标为1: 冬\n",
      "第47次sample的next_token下标为0: 小\n",
      "第48次sample的next_token下标为1: 冬\n",
      "第49次sample的next_token下标为1: 冬\n",
      "第50次sample的next_token下标为0: 小\n",
      "第51次sample的next_token下标为1: 冬\n",
      "第52次sample的next_token下标为1: 冬\n",
      "第53次sample的next_token下标为1: 冬\n",
      "第54次sample的next_token下标为1: 冬\n",
      "第55次sample的next_token下标为1: 冬\n",
      "第56次sample的next_token下标为1: 冬\n",
      "第57次sample的next_token下标为0: 小\n",
      "第58次sample的next_token下标为0: 小\n",
      "第59次sample的next_token下标为0: 小\n",
      "第60次sample的next_token下标为0: 小\n",
      "第61次sample的next_token下标为1: 冬\n",
      "第62次sample的next_token下标为0: 小\n",
      "第63次sample的next_token下标为1: 冬\n",
      "第64次sample的next_token下标为1: 冬\n",
      "第65次sample的next_token下标为1: 冬\n",
      "第66次sample的next_token下标为1: 冬\n",
      "第67次sample的next_token下标为0: 小\n",
      "第68次sample的next_token下标为0: 小\n",
      "第69次sample的next_token下标为1: 冬\n",
      "第70次sample的next_token下标为0: 小\n",
      "第71次sample的next_token下标为0: 小\n",
      "第72次sample的next_token下标为1: 冬\n",
      "第73次sample的next_token下标为1: 冬\n",
      "第74次sample的next_token下标为1: 冬\n",
      "第75次sample的next_token下标为1: 冬\n",
      "第76次sample的next_token下标为0: 小\n",
      "第77次sample的next_token下标为1: 冬\n",
      "第78次sample的next_token下标为0: 小\n",
      "第79次sample的next_token下标为1: 冬\n",
      "第80次sample的next_token下标为0: 小\n",
      "第81次sample的next_token下标为0: 小\n",
      "第82次sample的next_token下标为0: 小\n",
      "第83次sample的next_token下标为1: 冬\n",
      "第84次sample的next_token下标为0: 小\n",
      "第85次sample的next_token下标为1: 冬\n",
      "第86次sample的next_token下标为0: 小\n",
      "第87次sample的next_token下标为1: 冬\n",
      "第88次sample的next_token下标为0: 小\n",
      "第89次sample的next_token下标为0: 小\n",
      "第90次sample的next_token下标为1: 冬\n",
      "第91次sample的next_token下标为0: 小\n",
      "第92次sample的next_token下标为0: 小\n",
      "第93次sample的next_token下标为1: 冬\n",
      "第94次sample的next_token下标为0: 小\n",
      "第95次sample的next_token下标为0: 小\n",
      "第96次sample的next_token下标为0: 小\n",
      "第97次sample的next_token下标为1: 冬\n",
      "第98次sample的next_token下标为0: 小\n",
      "第99次sample的next_token下标为1: 冬\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "dict_map = {0: \"小\", 1: \"冬\", 2: \"瓜\"}\n",
    "prob = torch.tensor([0.3, 0.3, 0])\n",
    "print(\"prob : \", prob)\n",
    "\n",
    "print(\"根据概率权重所选择next token的下标\")\n",
    "for i in range(100):\n",
    "    next_token = torch.multinomial(prob, num_samples=1)[0]\n",
    "    print(f\"第{i}次sample的next_token下标为{next_token}:\",\n",
    "          dict_map[int(next_token)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Huggingface Transformer Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/head10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model_GPT = AutoModelForCausalLM.from_pretrained(\"/Volumes/WD_BLACK/models/gpt2\",\n",
    "                                                 pad_token_id=tokenizer.eos_token_id).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumonoultramicroscopicsilicovolcanoconiosis \n",
      "tensor([[   79, 25668,   261, 25955,   859,  2500,  1416,   404,   873, 41896,   709,   349,  5171,\n",
      "         36221, 42960,   220]])\n",
      "tensor(79)\n",
      "tensor(25668)\n",
      "tensor(261)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   79, 25668,   261, 25955,   859,  2500,  1416,   404,   873, 41896,   709,   349,  5171,\n",
      "        36221, 42960,   220,  1924,  1169,  2611,    11,   220,  1924,  1169,  2611,    11,   220,\n",
      "         1924,  1169,  2611,    11,   220,  1924,  1169,  2611,    11,   220,  1924,  1169,  2611,\n",
      "           11,   220,  1924,  1169,  2611,    11,   220,  1924,  1169,  2611,    11,   220,  1924,\n",
      "         1169,  2611,    11,   220,  1924,  1169,  2611,    11,   220,  1924,  1169,  2611,    11,\n",
      "          220])\n",
      "pneumonoultramicroscopicsilicovolcanoconiosis erythema, erythema, erythema, erythema, erythema, erythema, erythema, erythema, erythema, erythema, \n"
     ]
    }
   ],
   "source": [
    "text = \"pneumonoultramicroscopicsilicovolcanoconiosis \"\n",
    "# device = \"cuda:0\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs['input_ids'])\n",
    "print(inputs['input_ids'][0, 0])\n",
    "print(inputs['input_ids'][0, 1])\n",
    "print(inputs['input_ids'][0, 2])\n",
    "\n",
    "\n",
    "outputs = model_GPT.generate(**inputs, max_new_tokens=50)\n",
    "print(outputs[0])\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# Generate Greedy Search\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog',\n",
    "                         return_tensors='pt').to(torch_device)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model_GPT.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))  # skip_special_tokens=True, 去掉特殊token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# Generate Beam search\n",
    "# activate beam search and early_stopping\n",
    "beam_output = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n"
     ]
    }
   ],
   "source": [
    "# Generate Beam Searching\n",
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,  # 表示不重复的ngram大小\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea to\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time to take a\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea.\n"
     ]
    }
   ],
   "source": [
    "# Generate beam searching\n",
    "# set return_num_sequences > 1\n",
    "beam_outputs = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,  # 返回5个序列\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(\n",
    "      beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/head10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(13)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=False,  # 采样，False表示贪婪搜索，True表示采样\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, and I was delighted to have him on my show, so I had a chance to see him. I was very impressed with his body, and I am looking forward to seeing what he has to\n"
     ]
    }
   ],
   "source": [
    "# sampling temparature\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, which is a little unusual in this part of our family. He's a friendly, calm kind of dog, and I've always wanted to have him around, and I always wanted to go with\n"
     ]
    }
   ],
   "source": [
    "# Top-K Sampling\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, which is a little unusual in this part of our family. He's a friendly, calm, kind, caring person. He always makes us happy. The other dog I've gotten is an American\n"
     ]
    }
   ],
   "source": [
    "# Top-p (nucleus) sampling\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but there's another side to that. Because I am a single mom, and the fact that I am doing this kind of thing for my dog has caused me a lot of stress. Especially since\n",
      "1: I enjoy walking with my cute dog and always try not to let the dog get close to me. I would also suggest getting to know him better as he loves to be led by me and his paws which help in moving me. If\n",
      "2: I enjoy walking with my cute dog, and she's never been a shy, sweet, or carefree person in her life. She loves to be hugged, and I'm so sorry we never found that out.\"\n",
      "\n",
      "On Facebook\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(12)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model_GPT.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(\n",
    "      sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "head10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
